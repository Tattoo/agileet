#summary Theme task #2 for iteration 5
#sidebar TableOfContents

= Introduction =

Project followed [Process this process] and [Quality_assurance this quality assurance plan]. This document reviews how these worked and didn't work in relation to the product on Friday 24th of April 2009, the end day of the project.

= Synopsis =

Following quality goals were met: 
 # Unit testing has over 90% coverage and they all pass. This can be verified by running !AllTests -test suite with [http://eclemma.org/installation.html EclEmma-plugin] in Eclipse.
 # Release candidate was done weekly where all functionality was tested. Release candidate was successful every time, which was a nice bonus..
 # Code was heavily refactored to adhere standard Java codestyle. 
 # Refactoring was done heavily and CK-metrics provided framework to understand the quality of refactoring.
 # Project was peer-reviewed weekly. Pretty much all issues[`1`] were internally reviewed.
 # All issues were tracked in issue control system and was resolved one way or another. These were reviewed and customer should be aware of all decision on all issues.
 # Other documentation was updated and reflects the current state of the program.
 # Target platforms requirement (from [Requirement_specification requirement specification] Opera 9.64 and Firefox 3.0.7 on Windows XP Service pack 3, Ubuntu Linux 8.10, Mac OS X Tiger (10.4)) was met.
 
[`1`]: Some issues from before this demand came to effect were not

Following quality goals were not met:

 # Javadoc wasn't produced since documentation of code was lacking. Code doesen't have general documentation (eg. "what is purpose of class?"). 
 # No documentation to objectively show what CK-metrics say about the quality of code.


Notes:
 * *About quality problem 1*: On the other hand, code is clearly documented when it comes to questions about design decision made during implementation.

= Personal comments =

== Tatu ==

In general, I think the process and the quality plan worked well. There was *practically* no need to adjust the plans after they were first done. The process supported our way of working.

The hardest part about this project was discipline. Many times the thought "_this is just a course project_" creeped into mind when it was a matter of cranking a few more tests or procrastinating. It helped that you had to show what you did to other team members. 

Secondly, working remotely together is harder. Our process dropped a lot of agile methods relating to working together from our process and while it worked -- just and just, I feel -- for two people in a project this size, it is evident that eg. shared workspace and working same hours is very essential for efficient communication.

Third thing is, while I do believe our work is quite good qualitatively, we do not how much to show it if objective demonstration is needed. Our project could've benefitted from more metrics. On the other hand, it is clearly evident that eg. working hours is useless metric for our project[`1`], so the metrics we would (in imaginary future) choose, need to be carefully selected.

[`1`]: Since the project is about doing X hours per fixed 7 days, it is of no use to measure that. Separate issue is to measure how many issues were done in the sprint, or why some issue wasn't done.

== Joonas ==